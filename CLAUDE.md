# CLAUDE.md - Contributing to homelab-k8s-cluster

This document provides essential information for a code assistant to effectively contribute to this repository.

## Project Overview

This is an **Ansible playbook repository** for automating the deployment of a Kubernetes cluster in a home lab environment. The project uses Infrastructure as Code (IaC) principles to provision and configure a multi-node Kubernetes cluster with GitOps and distributed computing capabilities.

### Key Technologies
- **Ansible**: Configuration management and orchestration
- **Kubernetes**: Container orchestration platform (v1.28)
- **containerd**: Container runtime
- **Flannel**: CNI (Container Network Interface) plugin
- **Argo CD**: GitOps continuous delivery tool
- **Ray Operator**: Distributed computing framework for ML/AI workloads

### Architecture
- **1 Control Plane Node**: VM on Synology NAS (192.168.1.50)
- **5 Worker Nodes**: Physical systems (192.168.1.51-192.168.1.55)
- **Network**: 192.168.1.0/24 subnet, Pod network: 10.244.0.0/16

## Repository Structure

```
.
├── ansible.cfg              # Ansible configuration (SSH settings, defaults)
├── site.yml                 # Main playbook entry point
├── inventory/
│   └── hosts.yml           # Node inventory (IP addresses, groups)
├── group_vars/
│   └── all.yml             # Global variables (versions, config)
└── roles/
    ├── kubernetes-prerequisites/    # Install deps, container runtime
    ├── kubernetes-control-plane/    # Init cluster, install CNI
    ├── kubernetes-worker/           # Join workers to cluster
    ├── argocd/                     # Install and configure Argo CD
    └── ray-operator/               # Install Ray Operator
```

## Key Files and Their Purpose

### `site.yml`
The main playbook that orchestrates the entire cluster deployment in 5 phases:
1. Prerequisites (all nodes)
2. Control plane initialization
3. Worker node joining
4. Argo CD installation
5. Ray Operator installation

### `inventory/hosts.yml`
Defines the cluster topology with two groups:
- `control_plane`: Single master node
- `workers`: Five worker nodes
- `k8s_cluster`: Parent group containing both

### `group_vars/all.yml`
Central configuration for:
- `k8s_version`: Kubernetes version
- `pod_network_cidr`: Pod network range
- `ray_operator_version`: Ray Operator version
- `argocd_repo_url/branch/path`: GitOps repository settings (optional)

### Role Structure
Each role follows Ansible conventions:
- `roles/<role-name>/tasks/main.yml`: Task definitions
- Tasks use standard Ansible modules: `apt`, `command`, `shell`, `copy`, `template`, `wait_for`

## Making Changes

### Adding/Modifying Tasks

When working with Ansible tasks, follow these patterns:

1. **Idempotency**: Tasks should be safe to run multiple times
   ```yaml
   - name: Install package
     apt:
       name: package-name
       state: present
   ```

2. **Use appropriate modules**: Prefer declarative modules over `shell`/`command`
   - Use `apt` instead of `shell: apt-get install`
   - Use `copy` instead of `shell: cp`

3. **Handle errors gracefully**:
   ```yaml
   - name: Check if resource exists
     command: kubectl get nodes
     register: result
     ignore_errors: yes
   ```

4. **Use `become: yes`** for tasks requiring sudo (already set at playbook level)

### Version Updates

To update component versions:
1. Modify `group_vars/all.yml`
2. Update role tasks if installation commands change
3. Test on a single node first

### Adding New Roles

Create a new role structure:
```bash
mkdir -p roles/new-role/tasks
# Create roles/new-role/tasks/main.yml
```

Add to `site.yml`:
```yaml
- name: Install New Component
  hosts: control_plane  # or workers, or k8s_cluster
  become: yes
  roles:
    - new-role
```

## Common Patterns in This Repository

### 1. Kubeadm Cluster Initialization
The control plane role uses `kubeadm init` with specific parameters:
- `--pod-network-cidr`: Must match Flannel's network
- `--apiserver-advertise-address`: Control plane IP
- Join command is saved for worker nodes

### 2. Worker Node Joining
Workers execute the join command generated by the control plane:
- Join token and CA cert hash are required
- Command is typically stored/fetched during runtime

### 3. Kubectl Configuration
After cluster init, kubeconfig is copied to user's home directory:
```yaml
- name: Create .kube directory
  file:
    path: /home/{{ ansible_user }}/.kube
    state: directory
    
- name: Copy admin.conf
  copy:
    src: /etc/kubernetes/admin.conf
    dest: /home/{{ ansible_user }}/.kube/config
    remote_src: yes
```

### 4. Manifest Application
Kubernetes manifests are applied using `kubectl apply`:
```yaml
- name: Apply manifest
  shell: kubectl apply -f https://url/to/manifest.yaml
```

### 5. Waiting for Resources
Use `wait_for` or polling patterns:
```yaml
- name: Wait for pods to be ready
  shell: kubectl wait --for=condition=ready pod -l app=example -n namespace --timeout=300s
```

## Testing Considerations

### Pre-flight Checks
Before making changes, verify:
- Ansible syntax: `ansible-playbook site.yml --syntax-check`
- Connectivity: `ansible all -m ping`
- Dry run: `ansible-playbook site.yml --check` (limited effectiveness)

### Role-specific Testing
Test individual roles:
```bash
ansible-playbook site.yml --tags prerequisites
ansible-playbook site.yml --limit control_plane
```

### Post-deployment Validation
After changes, verify:
```bash
kubectl get nodes           # All nodes Ready
kubectl get pods -A         # All pods Running
kubectl get svc -n argocd   # Argo CD services
kubectl get pods -n kuberay-operator  # Ray Operator
```

## Common Tasks

### Update Kubernetes Version
1. Change `k8s_version` in `group_vars/all.yml`
2. Update apt repository URLs in `kubernetes-prerequisites` role
3. Run playbook on test node first

### Modify Network Configuration
1. Update `pod_network_cidr` in `group_vars/all.yml`
2. Update Flannel manifest URL/configuration in `kubernetes-control-plane` role
3. Requires cluster rebuild (not in-place upgrade)

### Add Argo CD Applications
1. Configure `argocd_repo_url`, `argocd_repo_branch`, `argocd_repo_path` in `group_vars/all.yml`
2. The argocd role will create an app-of-apps pattern Application
3. Ensure the repository structure matches expected format

### Add New Kubernetes Operators
1. Create new role under `roles/`
2. Add kubectl apply commands for operator manifests
3. Include namespace creation if needed
4. Add verification steps
5. Reference in `site.yml`

## Important Constraints

1. **Network addresses are hardcoded**: IP addresses in `inventory/hosts.yml` are specific to this home lab
2. **Single control plane**: This is not an HA setup
3. **Assumes Ubuntu/Debian**: Tasks use `apt` package manager
4. **SSH access required**: All nodes must be accessible via SSH with sudo privileges
5. **No rollback mechanism**: This is initial provisioning, not lifecycle management

## Variables to Customize

When helping users adapt this playbook:
- `inventory/hosts.yml`: IP addresses, hostnames, ansible_user
- `group_vars/all.yml`: Versions, network CIDRs, Argo CD repo settings
- `ansible.cfg`: SSH settings, privilege escalation

## Security Considerations

When making changes:
- Avoid hardcoding secrets in playbooks
- Suggest Ansible Vault for sensitive data
- Recommend changing default passwords (especially Argo CD admin)
- Consider firewall rules (not currently implemented)
- RBAC and network policies needed for production

## Debugging Tips

If tasks fail:
1. Check Ansible output for specific error messages
2. SSH to the failing node and check:
   - `journalctl -u kubelet -f` (Kubernetes)
   - `journalctl -u containerd -f` (Container runtime)
   - `kubectl get pods -A` (Pod status)
3. Verify network connectivity between nodes
4. Check `/var/log/` for application-specific logs

## GitOps Pattern (Argo CD)

If Argo CD variables are configured:
- The role creates a root Application pointing to the Git repository
- This Application should define child Applications (app-of-apps pattern)
- The repository structure should have an `apps/` directory with Application manifests

## Code Style

Maintain consistency:
- Use YAML syntax with 2-space indentation
- Use descriptive task names: `- name: Install containerd`
- Group related tasks with comments
- Use variables from `group_vars/all.yml` instead of hardcoding
- Follow Ansible best practices (idempotency, modules over shell)
